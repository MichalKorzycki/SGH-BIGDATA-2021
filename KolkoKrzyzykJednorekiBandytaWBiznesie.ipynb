{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acc3b972-1754-42c9-9fb3-cb0aa6f2d391",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# <font size=\"32\">Jak jednoręki bandyta i  „kółko i krzyżyk” </font>\n",
    "# <font size=\"32\">Mają zastosowanie w biznesie ? </font>\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "# Tematy na dziś:\n",
    "\n",
    "\n",
    "- ### Testy A/B\n",
    "- ### Dlaczego liczymy poziom __*istotności statystycznej*__\n",
    "- ### Moja checklista do testów A/B  \n",
    "- ### Gra w kółko i krzyżyk\n",
    "- ### Q-Learning i wzór Bellmana\n",
    "- ### Problem wielorękiego bandyty i *\"Bandit Algorithms\"*\n",
    "- ### Wyżarzanie\n",
    "<br>\n",
    "\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5711959a-4b6f-4a02-b123-7091c831b1b4",
   "metadata": {},
   "source": [
    "\n",
    "# Testy A/B\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "![](img\\jeff.jpg)\n",
    "<br>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df46141c-99ce-43e9-8901-2a3ff70b71fd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Za optimizely:\n",
    "\n",
    "### __*A/B testing*__ (also known as split testing or bucket testing) is a method of __*comparing two versions*__ of a webpage or app against each other to determine which one performs __*better*__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b44b8c7-881c-4f19-9721-7543bab6f2ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Co to znaczy __*better*__\n",
    "- Bounce rate\n",
    "- Conversion\n",
    "- Click-Through Rate\n",
    "- Etc. \n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "![](img\\danger.png)\n",
    "\n",
    "\n",
    "http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram\n",
    "<br>\n",
    "---\n",
    "\n",
    "# <font color='red'>__*DANGER ZONE 1*__</font>\n",
    "\n",
    "\n",
    "### __*better*__ zakłada, że jedno jest lepsze od drugiego.\n",
    "\n",
    "### A tak naprawdę są 3 możliwe odpowiedzi:\n",
    "- #### __*A*__ jest lepsze od __*B*__\n",
    "- #### __*B*__ jest lepsze od __*A*__\n",
    "- #### Wybór wersji __*A*__ czy __*B*__ __NIE MA WPŁYWU NA WYNIKI__\n",
    "\n",
    "---\n",
    "\n",
    "# Dlatego liczymy poziom __*istotności statystycznej*__\n",
    "\n",
    "## $H_0$ - hipoteza zerowa - oba zbiory mają tą samą średnią"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887e31fd-3ced-46a6-9f95-954ca696ad2b",
   "metadata": {},
   "source": [
    "### `scipy.stats.ttest_ind(a, b, axis=0, equal_var=True, nan_policy='propagate', alternative='two-sided')[source]`\n",
    "### Calculate the T-test for the means of two independent samples of scores.\n",
    "\n",
    "#### This is a two-sided test for the null hypothesis that 2 independent samples have identical average (expected) values. This test assumes that the populations have identical variances by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca36d25-bf3b-4ba0-9563-6326352bd25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from scipy import stats as st\n",
    "import random\n",
    "\n",
    "N=1000\n",
    "\n",
    "a = random.choices([0, 1],  weights=[980, 20], k=N)\n",
    "print(\"A :\", Counter(a))\n",
    "b = random.choices([0, 1],  weights=[980, 20], k=N)\n",
    "print(\"B :\", Counter(b))\n",
    "\n",
    "t, p = st.ttest_ind(a,b)\n",
    "print(f't={t} p={p}')\n",
    "print()\n",
    "\n",
    "if p>=0.05:\n",
    "    print(\"Nie można odrzucić hipotezy zerowej, że zbiory mają tą samą średnią\")\n",
    "else:\n",
    "    print(\"Należy odrzucić hipotezę zerową, że zbiory mają tą samą średnią\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9ecc12-9143-42aa-a296-2701bbc51d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats as st\n",
    "import random\n",
    "\n",
    "N=1000\n",
    "\n",
    "a = random.choices([0, 1],  weights=[980, 20], k=N)\n",
    "print(\"A :\", Counter(a))\n",
    "b = random.choices([0, 1],  weights=[975, 25], k=N)\n",
    "print(\"B :\", Counter(b))\n",
    "\n",
    "t, p = st.ttest_ind(a,b)\n",
    "print(f't={t} p={p}')\n",
    "print()\n",
    "\n",
    "if p>=0.05:\n",
    "    print(\"Nie można odrzucić hipotezy zerowej, że zbiory mają tą samą średnią\")\n",
    "else:\n",
    "    print(\"Należy odrzucić hipotezę zerową, że zbiory mają tą samą średnią\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb4b3ed-f3ca-40cf-bf4f-a5157faa85c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats as st\n",
    "import random\n",
    "\n",
    "N=10000\n",
    "\n",
    "a = random.choices([0, 1],  weights=[980, 20], k=N)\n",
    "print(\"A :\", Counter(a))\n",
    "b = random.choices([0, 1],  weights=[975, 25], k=N)\n",
    "print(\"B :\", Counter(b))\n",
    "\n",
    "t, p = st.ttest_ind(a,b)\n",
    "print(f't={t} p={p}')\n",
    "print()\n",
    "\n",
    "if p>=0.05:\n",
    "    print(\"Nie można odrzucić hipotezy zerowej, że zbiory mają tą samą średnią\")\n",
    "else:\n",
    "    print(\"Należy odrzucić hipotezę zerową, że zbiory mają tą samą średnią\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45061284-09dc-4b20-b4cc-09e3fa4482ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats as st\n",
    "import random\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "\n",
    "def ab(N, delta, w1=980, w2=20):\n",
    "    a = random.choices([0, 1],  weights=[w1, w2], k=N)\n",
    "    b = random.choices([0, 1],  weights=[w1-delta, w2+delta], k=N)\n",
    "    t, p = st.ttest_ind(a,b)\n",
    "    return p+0.000000001\n",
    "    \n",
    "\n",
    "experiments = range(200, 25_000, 200)\n",
    "tstats5 = [ ab(N, 5) for N in experiments]\n",
    "tstats10 = [ ab(N,10) for N in experiments]\n",
    "flatp5 = [ 0.05 for N in experiments ]    \n",
    "\n",
    "fig = plt.figure();\n",
    "plt.figure(figsize=(20,10));\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "\n",
    "plt.title('Poziom istotności statystycznej eksperymentu');\n",
    "t5, = plt.plot(experiments, tstats5, label=\"Eksperyment 2.0% vs 2.5%\");\n",
    "t10, = plt.plot(experiments, tstats10, label=\"Eksperyment 2.0% vs 3.0%\");\n",
    "p5, = plt.plot(experiments, flatp5, label=\"Poziom istotności 0.05\");\n",
    "plt.legend(handles=[t5, t10, p5], frameon=True, loc=\"best\",  fontsize=16);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7beaa4-7b42-4276-9227-1937e37f4aee",
   "metadata": {},
   "source": [
    "---\n",
    "# <font color='red'>__*DANGER ZONE 2*__</font>\n",
    "\n",
    "\n",
    "# Poziom istotności `p` ma sens _TYLKO_ w odniesieniu do _ZAKOŃCZONEGO_ eksperymentu\n",
    "\n",
    "# Przerwanie eksperymentu przed ustalonym a priori czasem jest nazywane \n",
    "- # `p-value fishing`\n",
    "- # `p-hacking`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e163bb0-a87b-4eb8-b2a0-7fcc3c9c3f79",
   "metadata": {},
   "source": [
    "---\n",
    "# Moja checklista do testów __A/B__\n",
    "\n",
    " - ### Testuj tylko jedną zmienną naraz (np. _CTR_)\n",
    " - ### Z góry ustal cel\n",
    " - ### <font color='yellow'>__Z góry ustal wielkość/granice testu__</font>\n",
    " - ### Z góry ustal poziom istotności i <font color='yellow'>NIE PRZERYWAJ WCZEŚNIEJ TESTU</font> po osiągnięciu tego poziomu\n",
    " - ### Ustal grupę kontrolną i grupę eksperymentalną\n",
    " - ### Podziel (w najprostszym przypadku) ruch na dwie części równe i <font color='yellow'>__LOSOWE__</font> \n",
    " - ### Testuj obie grupy <font color='yellow'>__RÓWNOCZESNIE__</font>\n",
    " - ### Jeśli tylko to możliwe, testuj wiele opcji:  *A/A*, *A/A/B*, *A/A/B/C*, *A/B/C/D* \n",
    "\n",
    "---\n",
    "\n",
    "## __A/A__ albo __A/A/B__ \n",
    "- ###  pozwalają sprawdzić czy system działa i stanowią dobrą podstawę do tłumaczenia się z poziomów istotności"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ec48e7-de24-4802-8c66-95c165599255",
   "metadata": {},
   "source": [
    "---\n",
    "# Gra w kółko i krzyżyk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e945f648-d7c0-4f29-8800-f02b827feb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell(x):\n",
    "    try:\n",
    "        if x.lower()==\"x\":\n",
    "            return \" X \"\n",
    "        if x.lower()==\"o\":\n",
    "            return \" O \"\n",
    "    except:\n",
    "        pass\n",
    "    return \"   \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714e476d-0027-4328-90cb-46608900fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state(state):\n",
    "    sep = \"_\"*11+\"\\n\"\n",
    "    ret = []\n",
    "    n=0\n",
    "    for row in state:\n",
    "        ret.append(\"|\".join(map(cell,row)))\n",
    "        n+=1\n",
    "        if n<3:\n",
    "                ret.append(sep)\n",
    "    for line in ret:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530352ba-3f14-45d1-a0a4-a02e5fc59632",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = (('x','o',0.0),\n",
    "         ('o','x',0.0),\n",
    "         (0.0,'o','x'))\n",
    "print_state(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d46722f-40ff-4f40-b248-4fecc960b83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_won(state):\n",
    "    players = ['x', 'o']\n",
    "    for i in [0,1]:\n",
    "        for row in state:\n",
    "            if row==tuple(players[i]*3): # _\n",
    "                return i, True\n",
    "        for cols in [0, 1, 2]:\n",
    "            if state[0][cols]==state[1][cols] and state[2][cols]==state[0][cols] and state[0][cols]==players[i]: # |\n",
    "                return i, True\n",
    "        if state[0][0]==state[1][1] and state[0][0]==state[2][2] and state[0][0]==players[i]:   # \\\n",
    "                return i, True\n",
    "        if state[2][0]==state[1][1] and state[2][0]==state[0][2] and state[0][2]==players[i]:   # /\n",
    "                return i, True\n",
    "            \n",
    "    return -1, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9304484d-51cb-4460-9109-6f663e2c7f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_full(state):\n",
    "    for row in state:\n",
    "        for v in row:\n",
    "            if v!=\"x\" and v!=\"o\":\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def is_valid_move(state, x, y):\n",
    "    row = state[x]\n",
    "    if row[y]!=\"x\" and row[y]!=\"o\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def new_state(state, x, y, character):\n",
    "    l = []\n",
    "    for row in state:\n",
    "        l.append(list(row))\n",
    "    l[x][y] = character\n",
    "    return (tuple(l[0]), tuple(l[1]), tuple(l[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ed7a0f-3763-4e24-97a4-d0152893713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Player:\n",
    "    def move(state):\n",
    "        return state\n",
    "    def learn(self, won):\n",
    "        pass\n",
    "\n",
    "    \n",
    "class HumanPlayer(Player):\n",
    "    def __init__(self, character):\n",
    "        self.character=character\n",
    "        \n",
    "    def move(self, state):\n",
    "        print_state(state)\n",
    "        flag = False\n",
    "        while not flag:\n",
    "            print()\n",
    "            x = int(input(f\"({self.character}) row: \"))\n",
    "            y = int(input(f\"({self.character}) col: \"))\n",
    "            flag=is_valid_move(state, x, y)\n",
    "        print()\n",
    "        \n",
    "        return new_state(state, x, y, self.character)\n",
    "\n",
    "    \n",
    "class RandomPlayer(Player):\n",
    "    def __init__(self, character):\n",
    "        self.character=character\n",
    "        \n",
    "    def move(self, state):\n",
    "        flag = False\n",
    "        while not flag:\n",
    "            x = random.randrange(3)\n",
    "            y = random.randrange(3)\n",
    "            flag=is_valid_move(state, x, y)\n",
    "       \n",
    "        return new_state(state, x, y, self.character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f986ad-4005-4634-8b6f-eb8061f5a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_game(player0, player1, stats, verbose=False):\n",
    "    state = ((0.0, 0.0, 0.0), (0.0, 0.0, 0.0), (0.0, 0.0, 0.0))\n",
    "    ended = False\n",
    "    full = False\n",
    "    won_won = -1\n",
    "    \n",
    "    while not ended and not full:\n",
    "        state = player0.move(state)\n",
    "        who_won, ended = has_won(state)\n",
    "        full = is_full(state)        \n",
    "        if not ended and not full:\n",
    "            state = player1.move(state)\n",
    "            who_won, ended = has_won(state)\n",
    "            full = is_full(state)\n",
    "    if ended:\n",
    "        stats.append(\"WIN 0\" if who_won==0 else \"WIN 1\")\n",
    "        player0.learn(1.0 if who_won==0 else 0.0)\n",
    "        player1.learn(1.0 if who_won==1 else 0.0)\n",
    "    if full and not ended:\n",
    "        stats.append(\"DRAW\")    \n",
    "        player0.learn(0.5)\n",
    "        player1.learn(0.5)\n",
    "    if verbose:\n",
    "        print(\"DRAW\" if full and not ended else f\"{who_won} has WON!\")\n",
    "        print()\n",
    "        print_state(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754ce503-f31b-424a-8e4a-8d8b98bb7d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "\n",
    "stats = []\n",
    "\n",
    "for i in range(1_000):\n",
    "    do_game(RandomPlayer('x'), RandomPlayer('o'), stats, False)\n",
    "    \n",
    "print(Counter(stats))\n",
    "\n",
    "\n",
    "def plot_games(qstats, label0, label1):\n",
    "    games = range(40, len(qstats), 20)\n",
    "    won0 = [ Counter(qstats[:x])['WIN 0']/x for x in games]\n",
    "    won1 = [ Counter(qstats[:x])['WIN 1']/x for x in games]\n",
    "    draw = [ Counter(qstats[:x])['DRAW']/x for x in games]\n",
    "\n",
    "\n",
    "    fig = plt.figure();\n",
    "    plt.figure(figsize=(20,10));\n",
    "    plt.ylim([0.0, 0.9])\n",
    "    plt.title('Results ratio');\n",
    "    player0, = plt.plot(games, won0, label=label0);\n",
    "    player1, = plt.plot(games, won1, label=label1);\n",
    "    draws, = plt.plot(games, draw, label=\"Draws\");\n",
    "    plt.legend(handles=[player0, player1,  draws], frameon=True, loc=\"best\",  fontsize=16);\n",
    "    plt.show();\n",
    "\n",
    "plot_games(stats, \"Random Player 0 Won\", \"Random Player 1 Won\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087cb52c-cb9a-4f32-ab3b-0fa75b2bcab9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Nauczanie Maszynowe to:\n",
    "\n",
    "- ### Uczenie z nadzorem (ang. *Supervised Learning*)\n",
    "- ### Uczenie bez nadzoru (ang. *Unsupervised Learning*)\n",
    "- ### __Uczenie ze wzmocnieniem (ang. *Reinforcement Learning*)__\n",
    "\n",
    "---\n",
    "# _*Q-Learning*_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2054829-4b47-43f0-847c-43cbf50a06b3",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Markov_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f1c0db-dd84-4a9f-973d-9f2b3588f0c5",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff438fd3-764b-41ff-bf34-397a670608be",
   "metadata": {},
   "source": [
    "$\n",
    "Q: S \\times A \\rightarrow I\\!R\n",
    "$\n",
    "---\n",
    "\n",
    "## W danym kroku $s$ podejmujemy akcję $a$ która maksymalizuje $Q$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed2c6da-7fd6-4433-ad6f-1149522196ed",
   "metadata": {},
   "source": [
    "\n",
    "## $Q$ Może zostać stabularyzowana (tzn. wsadzona do słownika)\n",
    "## Kluczami są pary `(stan, akcja)` a wartością wynik $Q$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860ea0c6-2335-4c99-8f27-db4ad8ae3433",
   "metadata": {},
   "source": [
    "---\n",
    "# Uczymy się $Q$ korzytając ze __*Wzoru Bellmana*__\n",
    "\n",
    "\n",
    "$\n",
    "Q^{new}(s_t, a_t) = Q(s_t, a_t) + \\alpha \\cdot ( r_t +  \\gamma \\cdot   \\max_{a}Q(s_{t+1}, a) - Q(s_t, a_t) )\n",
    "$\n",
    "---\n",
    "#### Upraszczając:\n",
    "\n",
    "$\n",
    "Q^{new}(s_t, a_t) = (1- \\alpha) \\cdot Q(s_t, a_t) + \\alpha \\cdot  \\gamma \\cdot   \\max_{a}Q(s_{t+1}, a)\n",
    "$\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### $\\alpha$ - Tempo uczenia się\n",
    "\n",
    "### $\\gamma$ - Degradacja wagi z odległością od bodźca\n",
    "\n",
    "### $r_t$ - nagroda w danym stanie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bd2449-81b6-4814-96e1-97161282373e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd45b98c-3d45-48e4-8931-c0e224fac2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "α = 0.15 # Tempo uczenia się\n",
    "β = 0.60 # Wartość początkowa `Q` - 'agresja'\n",
    "γ = 0.99 # Degradacja wagi z odległością od bodźca "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234aae5-5f92-44b6-a048-0d0efe0b86da",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28087a1-8b15-4431-966b-89de3507992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "\n",
    "class QPlayer(Player):\n",
    "    def __init__(self, character):\n",
    "        self.character=character\n",
    "        #################################################                     \n",
    "        ## stabularyzowane Q  postaci:                 ##\n",
    "        ## { stan1:                                    ##   \n",
    "        ##     { nowy_stan1: q1, nowy_stan2: q2, ...}, ##\n",
    "        ##   stan2:                                    ##\n",
    "        ##     { nowy_stan3: q3, nowy_stan4: q4, ...}, ##\n",
    "        ##  ... }                                      ##\n",
    "        #################################################                     \n",
    "        self.q_table={}\n",
    "        \n",
    "        self.previous_state=None\n",
    "        self.current_state=((0.0, 0.0, 0.0), (0.0, 0.0, 0.0), (0.0, 0.0, 0.0))\n",
    "        \n",
    "    def initialize_q_table(self, state):    \n",
    "        actions = {}   \n",
    "        for x in range(3):\n",
    "            for y in range(3):\n",
    "                if is_valid_move(state, x, y):\n",
    "                    ##################################\n",
    "                    ## Inicjalizacja przez β (beta) ##\n",
    "                    ##################################\n",
    "                    actions[\n",
    "                        new_state(state, x, y, self.character)\n",
    "                    ] = β\n",
    "        self.q_table[state] = actions\n",
    "        \n",
    "        return actions\n",
    "        \n",
    "    def move(self, state):\n",
    "        \n",
    "        ##############################################################\n",
    "        ## Tworzę powiązanie z ruchem drugiego gracza,              ##\n",
    "        ## Aby w Q-table był ciągły łańcuch od poprzedniego ruchu   ##\n",
    "        ##############################################################\n",
    "        \n",
    "        if self.previous_state:\n",
    "            actions = self.q_table.get(self.current_state, {})\n",
    "            if state not in actions.keys():\n",
    "                actions[state] = β\n",
    "                self.q_table[self.current_state] = actions\n",
    "            \n",
    "        ##############################################################\n",
    "        ## Jeśli w Q-table nie ma akcji powiązanych z obecnym       ##\n",
    "        ## stanem, dopisuje wszystkie możliwe akcje z wagą β (beta) ##\n",
    "        ##############################################################\n",
    "        actions = self.q_table.get(state)\n",
    "        if not actions:\n",
    "            actions = self.initialize_q_table(state)\n",
    "\n",
    "        ##############################################################\n",
    "        ## Biorę najlepsze Q dla akcji w tym stanie                 ##\n",
    "        ##############################################################\n",
    "        best_q = max(actions.values())\n",
    "        \n",
    "        ##############################################################\n",
    "        ## Losuję akcję pośród tych co mają najlepsze Q             ##\n",
    "        ##############################################################\n",
    "        best_actions = [ action for action, q in actions.items() if q==best_q ]\n",
    " \n",
    "        self.previous_state = state\n",
    "        self.current_state = sample(best_actions, 1)[0]\n",
    "\n",
    "        return self.current_state\n",
    "    \n",
    "    def learn(self, learned_weight):\n",
    "        \n",
    "        self.q_table[self.previous_state][self.current_state] = learned_weight\n",
    "        self.previous_state=None\n",
    "        \n",
    "        for state, actions in self.q_table.items():\n",
    "            for next_move, q in actions.items():\n",
    "                next_move_actions = self.q_table.get(next_move)\n",
    "                if next_move_actions:\n",
    "                    best_next_q = max(next_move_actions.values())\n",
    "                   \n",
    "                    #########################\n",
    "                    ##    Wzór Bellmana    ##\n",
    "                    #########################\n",
    "                    actions[next_move] = (1-α)*q + γ*α*best_next_q \n",
    "                    \n",
    "                    self.q_table[state] = actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcdcf33-03f6-4fb2-9fef-f390b4145877",
   "metadata": {},
   "outputs": [],
   "source": [
    "qstats = []\n",
    "qplayer = QPlayer('o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db73263-9b59-4a89-83f0-e583db33d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for i in range(1):\n",
    "    do_game(RandomPlayer('x'), qplayer, qstats, False)\n",
    "    \n",
    "print(Counter(qstats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c5310a-aeb8-4c72-929a-9c32901a55fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint \n",
    "pprint(qplayer.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d66228-b443-4b28-8aa3-9f1b12d0db26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2_000):\n",
    "    do_game(RandomPlayer('x'), qplayer,  qstats, False)\n",
    "    \n",
    "print(Counter(qstats))\n",
    "plot_games(qstats, \"Random Player 0 Won\", \"Q Player 1 Won\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d74078-8f12-407d-b17e-2585e0b7965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2_000):\n",
    "    do_game(RandomPlayer('x'), qplayer,  qstats, False)\n",
    "    \n",
    "print(Counter(qstats))\n",
    "plot_games(qstats, \"Random Player 0 Won\", \"Q Player 1 Won\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb36696-2a6d-40dd-b0ae-3d064ba61cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2_game(qstats, stats, labelA, labelB):\n",
    "    qgames = range(40, len(qstats), 20)\n",
    "    qwon0 = [ Counter(qstats[:x])['WIN 0']/x for x in qgames]\n",
    "    qwon1 = [ Counter(qstats[:x])['WIN 1']/x for x in qgames]\n",
    "    qdraw = [ Counter(qstats[:x])['DRAW']/x for x in qgames]\n",
    "    games = range(40, len(stats), 20)\n",
    "    won0 = [ Counter(stats[:x])['WIN 0']/x for x in games]\n",
    "    won1 = [ Counter(stats[:x])['WIN 1']/x for x in games]\n",
    "    draw = [ Counter(stats[:x])['DRAW']/x for x in games]\n",
    "\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = [20,10]\n",
    " \n",
    "    fig = plt.figure();\n",
    " \n",
    "    l = fig.add_subplot(121)   \n",
    "    plt.ylim([0.0, 0.9])\n",
    "    player0, = l.plot(qgames, qwon0, label=\"Random Player 0\");\n",
    "    player1, = l.plot(qgames, qwon1, label=labelA);\n",
    "    draws, = l.plot(qgames, qdraw, label=\"Draws\");\n",
    "    l.legend(handles=[player0, player1,  draws], frameon=True, loc=\"best\", fontsize=16);\n",
    "\n",
    "    r = fig.add_subplot(122)  \n",
    "    plt.ylim([0.0, 0.9])\n",
    "    player0, = r.plot(games, won0, label=\"Random Player 0\");\n",
    "    player1, = r.plot(games, won1, label=labelB);\n",
    "    draws, = r.plot(games, draw, label=\"Draws\");\n",
    "    r.legend(handles=[player0, player1,  draws], frameon=True, loc=\"best\", fontsize=16);\n",
    "    plt.show();\n",
    "\n",
    "plot_2_game(qstats, stats, \"Q-Player 1\", \"Random Player 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed58e8-cb17-4faf-9ecf-9cd163137d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "qstats2 = []\n",
    "for i in range(1_000):\n",
    "    do_game(RandomPlayer('x'), qplayer,  qstats2, False)\n",
    "    \n",
    "print(Counter(qstats2))\n",
    "plot_games(qstats2, \"Random Player 0 Won\", \"Q Player 1 Won\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466ed171-c181-44a9-b8e4-884d8f1a5d45",
   "metadata": {},
   "source": [
    "---\n",
    "## A/B Testing ma zazwyczaj dwie fazy\n",
    "- ### __*Eksploracji*__ - sprawdzamy która lepsza jest lepsza\n",
    "- ### __*Eksploatacji*__ - \"lepsza\" wersja działa jako jedyna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9093235-ce7c-4da5-8835-df872086cbe7",
   "metadata": {},
   "source": [
    "## Wady typowego podejścia\n",
    "- ### Nie ma płynnego przejścia z 2 wersji na 1\n",
    "- ### W czasie eksploracji sprawdzamy wiele dużo wersji gorszych od wersji kontrolnej\n",
    "- ### __*Eksploracja w praktyce nigdy się nie kończy*__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1955f337-8229-4cd5-91ba-3e77c5b425ff",
   "metadata": {},
   "source": [
    "---\n",
    "# Problem Wielorękiego Bandyty\n",
    "\n",
    "![](img\\Las_Vegas_slot_machines.jpg)\n",
    "\n",
    "https://en.wikipedia.org/wiki/Multi-armed_bandit\n",
    "\n",
    "Mamy budżet aby jak najwięcej \"ugrać\". Poszczególni jednoręcy bandyci różnią się między sobą szansą na wygraną. \n",
    "\n",
    "Ile spędzić budzetu na eksplorację najlepszej maszyny, a ile na osiągnięcie wygranej ?\n",
    "\n",
    "Istnieje olbrzymi katalog algorytmów (ang. __bandit algorithms__) które na to odpowiadają.\n",
    "- ε-greedy\n",
    "- soft-max\n",
    "- soft-max z wyżarzaniem\n",
    "- Upper Confidence Bound\n",
    "- Exp3\n",
    "- etc. etc.\n",
    "\n",
    "![](img\\banditalgos.jpg)\n",
    "\n",
    "---\n",
    "## Algorytm ε-zachłanny - ε-greedy\n",
    "\n",
    "- #### Z prawdopodobieństwem `1-ε`  wybieramy strategię kontrolną `A` do eksploatacji. \n",
    "\n",
    "- #### Z prawdopodobieństwem `ε` (np. 20%) wybieramy strategię `B`  do eksploracji. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c80a02-5126-45a9-8159-a1cf628d9167",
   "metadata": {},
   "source": [
    "![](img\\epsilongreedy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0df6c89-bce8-4004-8802-f0e66248aa01",
   "metadata": {},
   "source": [
    "---\n",
    "## Algorytm Soft-max\n",
    "\n",
    "#### $r_A$ - proporcja sukcesów w gałęzi `A`\n",
    "\n",
    "#### $r_B$ - proporcja sukcesów w gałęzi `B`\n",
    "\n",
    "<br>\n",
    "\n",
    "- #### Z prawdopodobieństwem $$\\frac{r_A}{r_A+r_B}$$ wybieramy strategię kontrolną `A` do eksploatacji. \n",
    "\n",
    "- #### Z prawdopodobieństwem $$\\frac{r_B}{r_A+r_B}$$ wybieramy strategię `B`  do eksploracji. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fd4dee-1efd-4ca8-8d25-6e1a658df40d",
   "metadata": {},
   "source": [
    "---\n",
    "## Algorytm Soft-max z wyżarzaniem\n",
    "\n",
    "#### $r_A$ - proporcja sukcesów w gałęzi `A`\n",
    "\n",
    "#### $r_B$ - proporcja sukcesów w gałęzi `B`\n",
    "\n",
    "#### $\\tau$ - \"temperatura\"\n",
    "\n",
    "<br>\n",
    "\n",
    "- #### Z prawdopodobieństwem $$\\frac{exp(r_A/\\tau)}{exp(r_A/\\tau)+exp(r_B/\\tau)}$$ wybieramy strategię kontrolną `A` do eksploatacji. \n",
    "\n",
    "- #### Z prawdopodobieństwem $$\\frac{exp(r_B/\\tau)}{exp(r_A/\\tau)+exp(r_B/\\tau)}$$ wybieramy strategię `B`  do eksploracji. \n",
    "\n",
    "---\n",
    "`t = sum(self.counts) + 1`\n",
    "\n",
    "`tau = 1 / math.log(t + 0.0000001)`\n",
    "\n",
    "\n",
    "---\n",
    "## __*Wyżarzanie*__\n",
    "\n",
    "### __*Wyżarzanie*__ - to obniżanie \"temperatury\" czyli zmienności układu z czasem\n",
    "\n",
    "### Dzięki __*wyżarzaniu*__ - system z czasem coraz rzadziej eksploruje.\n",
    "\n",
    "#### __*Hartowanie*__ to zwiększanie temperatury raz na jakiś czas aby \"wybić\" system z minimów lokalnych \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c058f3bb-d437-44cd-82dd-44a76b1e6ff7",
   "metadata": {},
   "source": [
    "---\n",
    "## Q-Player z __*Wyżarzaniem*__\n",
    "\n",
    "Zmniejszamy `α` przez mnożenie przez `γ` co krok - przez co spada zmienność układu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11ad3c0-ba42-4d70-b6e2-2b14b69a7e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "α = 0.99  # Tempo uczenia się - \"temperatura\"\n",
    "β = 0.80  # Wartość początkowa `Q` - 'agresja'\n",
    "γ = 0.975 # Degradacja wagi z odległością od bodźca oraz tempo spadku \"temperatury\"\n",
    "\n",
    "class AnnealingQPlayer(QPlayer):\n",
    "    def __init__(self, character):\n",
    "        super().__init__(character)\n",
    "        \n",
    "    def learn(self, learned_weight):\n",
    "        global α\n",
    "        \n",
    "        ################\n",
    "        ## Wyżarzanie ##\n",
    "        ################\n",
    "        α *= γ\n",
    "        \n",
    "        super().learn(learned_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb69d60-e8d5-4d5b-81df-f1c4a76d5c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "annealing_qplayer = AnnealingQPlayer('o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c290762-a528-4119-bf7f-8780e4216db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aqstats = []\n",
    "for i in range(2_000):\n",
    "    do_game(RandomPlayer('x'), annealing_qplayer,  aqstats, False)\n",
    "    \n",
    "print(Counter(aqstats))\n",
    "plot_games(aqstats, \"Random Player 0 Won\", \"Annealing Q Player 1 Won\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416cf20f-28b9-4bf8-8a9b-d4d652eed707",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2_000):\n",
    "    do_game(RandomPlayer('x'), annealing_qplayer,  aqstats, False)\n",
    "    \n",
    "print(Counter(aqstats))\n",
    "plot_games(aqstats, \"Random Player 0 Won\", \"Annealing Q Player 1 Won\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95341d10-ead7-46a5-92c5-ce573d4fe49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2_game(qstats, aqstats, \"Q-Player 1\", \"Annealing Q-Player 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c640618d-f539-4604-94bc-63dd8578aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aqstats2 = []\n",
    "for i in range(1_000):\n",
    "    do_game(RandomPlayer('x'), annealing_qplayer,  aqstats2, False)\n",
    "    \n",
    "print(Counter(aqstats2))\n",
    "\n",
    "\n",
    "plot_2_game(qstats2, aqstats2, \"Q-Player 1\", \"Annealing Q-Player 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94641d69-6378-41ff-998a-00a63b976c83",
   "metadata": {},
   "source": [
    "---\n",
    "# Materiały\n",
    "\n",
    "- *p-Hacking and False Discovery in A/B Testing* - Ron Berman, Leonid Pekelisy, Aisling Scottz, Christophe Van den Bultex. December 11, 2018\n",
    "\n",
    "https://www.msi.org/working-papers/phacking-and-false-discovery-in-a-b-testing-2/\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Q-learning\n",
    "\n",
    "- https://www.twitch.tv/michal_korzycki\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6458e1e6-65d2-4566-ac1e-14cde9ffde55",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_game(HumanPlayer('x'), annealing_qplayer, [], True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
